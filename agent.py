import os
import time
import logging
import hashlib
from datetime import datetime, timedelta
from functools import lru_cache
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate

# å¯¼å…¥é…ç½®
from config import PATHS, MODEL_CONFIG, RETRIEVAL_CONFIG, CHROMA_CONFIG, LOGGING_CONFIG, CACHE_CONFIG, CONVERSATION_CONFIG

# é…ç½®æ—¥å¿—
log_file = os.path.join(LOGGING_CONFIG["log_file"], f"agent_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
logging.basicConfig(
    level=LOGGING_CONFIG["level"],
    format=LOGGING_CONFIG["format"],
    encoding=LOGGING_CONFIG["encoding"],
    handlers=[
        logging.FileHandler(log_file, encoding="utf-8"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ç¼“å­˜ç³»ç»Ÿ
class SimpleCache:
    """ç®€å•çš„å†…å­˜ç¼“å­˜ç³»ç»Ÿ"""
    def __init__(self, max_size=100, ttl=3600):
        self.max_size = max_size
        self.ttl = ttl
        self.cache = {}
        
    def _generate_key(self, question):
        """ç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.md5(question.encode()).hexdigest()
    
    def get(self, question):
        """è·å–ç¼“å­˜å€¼"""
        key = self._generate_key(question)
        if key in self.cache:
            value, timestamp = self.cache[key]
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            if datetime.now() - timestamp < timedelta(seconds=self.ttl):
                logger.debug(f"ç¼“å­˜å‘½ä¸­: {question[:30]}...")
                return value
            else:
                # è¿‡æœŸåˆ é™¤
                del self.cache[key]
                logger.debug(f"ç¼“å­˜è¿‡æœŸ: {question[:30]}...")
        return None
    
    def set(self, question, answer):
        """è®¾ç½®ç¼“å­˜å€¼"""
        # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œåˆ é™¤æœ€æ—©çš„é¡¹ç›®
        if len(self.cache) >= self.max_size:
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]
            logger.debug(f"ç¼“å­˜å·²æ»¡ï¼Œåˆ é™¤æœ€æ—©é¡¹")
        
        key = self._generate_key(question)
        self.cache[key] = (answer, datetime.now())
        logger.debug(f"ç¼“å­˜è®¾ç½®: {question[:30]}...")

# åˆ›å»ºç¼“å­˜å®ä¾‹
if CACHE_CONFIG["enabled"]:
    cache = SimpleCache(max_size=CACHE_CONFIG["max_size"], ttl=CACHE_CONFIG["ttl"])
else:
    cache = None

# æ·»åŠ è¿›åº¦æ˜¾ç¤ºå‡½æ•°
def show_progress(message, step=None, total=None):
    """æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯ï¼ŒåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ—¥å¿—"""
    if step and total:
        progress = f"[{step}/{total}] "
    else:
        progress = ""
    full_message = f"{progress}{message}"
    print(f"[è¿›åº¦] {full_message}")
    logger.info(full_message)

# è‡ªå®šä¹‰æç¤ºæ¨¡æ¿
DEFAULT_PROMPT_TEMPLATE = """
ã€æŒ‡ä»¤ã€‘
ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼éµå¾ªæ–‡æ¡£çš„RAGé—®ç­”ç³»ç»Ÿï¼Œè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤å¤„ç†é—®é¢˜ï¼š

1. åˆ†æé˜¶æ®µï¼š
   - è¯†åˆ«é—®é¢˜ç±»å‹ï¼ˆäº‹å®æŸ¥è¯¢/é€»è¾‘æ¨ç†/æ— å…³é—®é¢˜ï¼‰
   - æ£€æŸ¥æ–‡æ¡£ç›¸å…³æ€§ï¼ˆå®Œå…¨ç›¸å…³/éƒ¨åˆ†ç›¸å…³/ä¸ç›¸å…³ï¼‰

2. å›ç­”è§„åˆ™ï¼š
   - å½“æ–‡æ¡£å®Œå…¨è¦†ç›–é—®é¢˜æ—¶ï¼šç»¼åˆå¤šä¸ªæ–‡æ¡£ç‰‡æ®µç»™å‡ºç²¾å‡†å›ç­”
   - å½“æ–‡æ¡£éƒ¨åˆ†ç›¸å…³æ—¶ï¼š
     * åªåŸºäºæ–‡æ¡£å†…å®¹å›ç­”å·²çŸ¥éƒ¨åˆ†
     * æ˜ç¡®è¯´æ˜æ–‡æ¡£çš„å±€é™å¤„
   - å½“æ–‡æ¡£ä¸ç›¸å…³æ—¶ï¼šä¸¥æ ¼æ‹’ç»å›ç­”

3. æ ¼å¼è¦æ±‚ï¼š
   - ä½¿ç”¨ã€å·²çŸ¥ä¿¡æ¯ã€‘ã€ã€æ–‡æ¡£ä¾æ®ã€‘ç­‰æ ‡è®°è¯´æ˜æ¥æº
   - ä½¿ç”¨ä¸­æ–‡ä¹¦é¢è¯­ï¼Œä¿æŒä¸“ä¸šä½†æ˜“æ‡‚
   - åˆ†ç‚¹è®ºè¿°å¤æ‚é—®é¢˜ï¼Œé‡è¦ä¿¡æ¯ä¼˜å…ˆ

ã€æ–‡æ¡£å†…å®¹ã€‘
{context}

ã€å¾…å›ç­”é—®é¢˜ã€‘
{question}

ã€ç³»ç»Ÿå“åº”ã€‘
"""

# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
@lru_cache(maxsize=1)
def initialize_embeddings():
    """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆä½¿ç”¨lru_cacheç¼“å­˜å®ä¾‹ï¼‰"""
    try:
        show_progress(f"å¼€å§‹åŠ è½½åµŒå…¥æ¨¡å‹: {MODEL_CONFIG['embedding_model']}")
        
        # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(PATHS["model_path"]):
            logger.warning(f"è­¦å‘Š: æ¨¡å‹ç¼“å­˜è·¯å¾„ {PATHS['model_path']} ä¸å­˜åœ¨ï¼Œå°†å°è¯•ä¸‹è½½")
        
        # åŠ è½½æœ¬åœ°åµŒå…¥æ¨¡å‹
        embeddings = HuggingFaceEmbeddings(
            model_name=MODEL_CONFIG["embedding_model"],
            cache_folder=PATHS["model_path"],
            model_kwargs={'device': MODEL_CONFIG.get('device', 'cpu')},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        show_progress("åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
        return embeddings
    except Exception as e:
        logger.error(f"åŠ è½½åµŒå…¥æ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
        raise

# å‘é‡æ•°æ®åº“ç›¸å…³å‡½æ•°
def load_vector_db(embeddings):
    """åŠ è½½å‘é‡æ•°æ®åº“"""
    try:
        show_progress(f"å¼€å§‹åŠ è½½çŸ¥è¯†åº“: {PATHS['chroma_data']}")
        
        # æ£€æŸ¥æ•°æ®åº“è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(PATHS["chroma_data"]):
            raise FileNotFoundError(f"çŸ¥è¯†åº“è·¯å¾„ä¸å­˜åœ¨: {PATHS['chroma_data']}\nè¯·å…ˆè¿è¡Œrag_init.pyåˆå§‹åŒ–çŸ¥è¯†åº“")
        
        # ä½¿ç”¨æ›´å…¼å®¹çš„æ–¹å¼åŠ è½½Chromaæ•°æ®åº“
        db = Chroma(
            persist_directory=PATHS["chroma_data"],
            embedding_function=embeddings,
            collection_name=CHROMA_CONFIG.get("collection_name", "documents")
        )
        
        # å°è¯•è·å–é›†åˆå†…å®¹ï¼Œä½¿ç”¨æ›´çµæ´»çš„æ£€æŸ¥æ–¹å¼
        try:
            # å…ˆå°è¯•ç›´æ¥è·å–æ–‡æ¡£
            documents = db.get()
            if not documents.get('ids'):
                # å¯èƒ½æ˜¯é›†åˆåç§°é—®é¢˜ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤é›†åˆ
                db = Chroma(
                    persist_directory=PATHS["chroma_data"],
                    embedding_function=embeddings
                )
                documents = db.get()
                
            if documents.get('ids'):
                show_progress(f"çŸ¥è¯†åº“åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(documents['ids'])} ä¸ªå‘é‡")
                return db
            else:
                raise ValueError(f"çŸ¥è¯†åº“ä¸ºç©º: {PATHS['chroma_data']}\nè¯·æ£€æŸ¥æ–‡æ¡£æ˜¯å¦æ­£ç¡®åŠ è½½")
        except Exception as inner_e:
            logger.warning(f"ç›´æ¥æ£€æŸ¥é›†åˆæ—¶å‡ºé”™: {str(inner_e)}ï¼Œå°è¯•æ›¿ä»£æ–¹æ³•")
            # æ›¿ä»£æ–¹æ³•ï¼šå°è¯•æŸ¥è¯¢æ“ä½œéªŒè¯æ•°æ®åº“
            try:
                # å°è¯•ä¸€ä¸ªç®€å•æŸ¥è¯¢æ¥éªŒè¯æ•°æ®åº“
                results = db.similarity_search("æµ‹è¯•æŸ¥è¯¢", k=1)
                if results:
                    show_progress(f"çŸ¥è¯†åº“åŠ è½½æˆåŠŸ")
                    return db
                else:
                    raise ValueError(f"çŸ¥è¯†åº“ä¸ºç©ºæˆ–æ— æ³•æ­£å¸¸æŸ¥è¯¢: {PATHS['chroma_data']}")
            except Exception as query_e:
                logger.error(f"æŸ¥è¯¢éªŒè¯å¤±è´¥: {str(query_e)}")
                raise ValueError(f"åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {PATHS['chroma_data']}\nè¯¦ç»†é”™è¯¯: {str(query_e)}")
    except Exception as e:
        logger.error(f"åŠ è½½çŸ¥è¯†åº“æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
        raise

# åˆå§‹åŒ–è¯­è¨€æ¨¡å‹
@lru_cache(maxsize=1)
def initialize_llm():
    """åˆå§‹åŒ–è¯­è¨€æ¨¡å‹ï¼ˆä½¿ç”¨lru_cacheç¼“å­˜å®ä¾‹ï¼‰"""
    try:
        show_progress(f"å¼€å§‹åˆå§‹åŒ–LLM: {MODEL_CONFIG['ollama_model']}")
        
        # åŠ è½½ Ollama æ¨¡å‹
        llm = Ollama(
            model=MODEL_CONFIG["ollama_model"],
            temperature=MODEL_CONFIG["temperature"],
            num_predict=MODEL_CONFIG.get("max_tokens", 1024),
            # æ·»åŠ è¶…æ—¶è®¾ç½®
            # timeout=60.0
            timeout=300.0

        )
        
        # æµ‹è¯•æ¨¡å‹è¿æ¥
        try:
            # ä½¿ç”¨ä¸€ä¸ªç®€å•çš„æç¤ºæµ‹è¯•æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ
            test_response = llm.invoke("è¯·è¯´ä¸€å£°'ä½ å¥½'ç¡®è®¤è¿æ¥æ­£å¸¸")
            if "ä½ å¥½" in test_response:
                show_progress("LLMåˆå§‹åŒ–æˆåŠŸå¹¶è¿æ¥æ­£å¸¸")
            else:
                logger.warning(f"LLMæµ‹è¯•å“åº”å¼‚å¸¸: {test_response}")
        except Exception as test_error:
            logger.warning(f"LLMè¿æ¥æµ‹è¯•å¤±è´¥ï¼Œä½†å°†ç»§ç»­å°è¯•: {str(test_error)}")
        
        return llm
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–LLMæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
        raise

# åˆ›å»ºä¼šè¯å†…å­˜
def create_conversation_memory():
    """åˆ›å»ºä¼šè¯å†…å­˜"""
    try:
        memory = ConversationBufferMemory(
            memory_key="chat_history",
            input_key="query",  # ä¿®æ”¹ä¸ºä¸promptæ¨¡æ¿ä¸€è‡´çš„é”®å
            output_key="result",
            k=CONVERSATION_CONFIG["max_history_length"]
        )
        show_progress(f"ä¼šè¯å†…å­˜åˆ›å»ºæˆåŠŸï¼Œæœ€å¤§å†å²è®°å½•é•¿åº¦: {CONVERSATION_CONFIG['max_history_length']}")
        return memory
    except Exception as e:
        logger.error(f"åˆ›å»ºä¼šè¯å†…å­˜æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
        # è¿”å›Noneè¡¨ç¤ºä½¿ç”¨æ— ä¼šè¯æ¨¡å¼
        return None

# åˆ›å»ºé—®ç­”é“¾
def create_qa_chain(db, llm, memory=None):
    """åˆ›å»ºé—®ç­”é“¾"""
    try:
        show_progress("å¼€å§‹åˆ›å»ºé—®ç­”é“¾")
        
        # åˆ›å»ºæ£€ç´¢å™¨
        retriever = db.as_retriever(
            search_type=RETRIEVAL_CONFIG["search_type"],
            search_kwargs={
                "k": RETRIEVAL_CONFIG["k"],
                **RETRIEVAL_CONFIG.get("search_kwargs", {})
            }
        )
        
        # åˆ›å»ºæç¤ºæ¨¡æ¿
        prompt = PromptTemplate(
            template=DEFAULT_PROMPT_TEMPLATE,
            input_variables=["context", "question"]
        )
        
        # æ„å»º QA é“¾
        chain_kwargs = {
            "llm": llm,
            "chain_type": "stuff",
            "retriever": retriever,
            "return_source_documents": True,
            "chain_type_kwargs": {
                "prompt": prompt
            }
        }
        
        # å¦‚æœæä¾›äº†å†…å­˜ï¼Œæ·»åŠ åˆ°é“¾ä¸­
        if memory:
            chain_kwargs["memory"] = memory
        
        qa_chain = RetrievalQA.from_chain_type(**chain_kwargs)
        
        show_progress("é—®ç­”é“¾åˆ›å»ºæˆåŠŸ")
        return qa_chain
    except Exception as e:
        logger.error(f"åˆ›å»ºé—®ç­”é“¾æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
        raise

# æ‰§è¡Œæ–‡æ¡£æ£€ç´¢
def perform_retrieval(retriever, question):
    """æ‰§è¡Œæ–‡æ¡£æ£€ç´¢"""
    try:
        show_progress(f"å¼€å§‹æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼Œé—®é¢˜: {question[:50]}...")
        
        # æ‰§è¡Œæ£€ç´¢
        relevant_docs = retriever.get_relevant_documents(question)
        show_progress(f"æ£€ç´¢åˆ° {len(relevant_docs)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
        
        # è®°å½•æ£€ç´¢ç»“æœ
        for i, doc in enumerate(relevant_docs, 1):
            source = doc.metadata.get("source", "æœªçŸ¥æ¥æº")
            logger.debug(f"ç¬¬ {i} ä¸ªç‰‡æ®µæ¥æº: {source}ï¼Œå†…å®¹å‰100å­—ç¬¦: {doc.page_content[:100]}...")
        
        if not relevant_docs:
            logger.warning("è­¦å‘Š: æœªæ£€ç´¢åˆ°ä»»ä½•ç›¸å…³æ–‡æ¡£")
        
        return relevant_docs
    except Exception as e:
        logger.error(f"æ£€ç´¢ç›¸å…³æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
        raise

# æµå¼ç”Ÿæˆå›ç­”
def generate_answer_stream(qa_chain, question):
    """æµå¼ç”Ÿæˆå›ç­”"""
    try:
        show_progress("æ­£åœ¨è°ƒç”¨æ¨¡å‹ç”Ÿæˆå›ç­”ï¼ˆæµå¼è¾“å‡ºï¼‰...")
        
        # ä½¿ç”¨æµå¼è°ƒç”¨ï¼Œç¡®ä¿é”®åä¸promptæ¨¡æ¿åŒ¹é…
        for chunk in qa_chain.stream({"query": question}):
            if isinstance(chunk, dict) and "result" in chunk:
                yield chunk["result"]
    except Exception as e:
        logger.error(f"æµå¼ç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
        yield f"ç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"

# ç”Ÿæˆå›ç­”
def generate_answer(qa_chain, question):
    """ç”Ÿæˆå›ç­”"""
    try:
        show_progress("æ­£åœ¨è°ƒç”¨æ¨¡å‹ç”Ÿæˆå›ç­”ï¼ˆå¯èƒ½éœ€è¦å‡ ç§’é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...ï¼‰")
        
        # æ·»åŠ æ€è€ƒæ—¶é—´çš„è§†è§‰åé¦ˆ
        start_time = time.time()
        
        # ä½¿ç”¨æ­£ç¡®çš„é”®åè°ƒç”¨
        result = qa_chain.invoke({"query": question})
        
        end_time = time.time()
        show_progress(f"å›ç­”ç”Ÿæˆå®Œæˆï¼Œç”¨æ—¶: {round(end_time - start_time, 2)}ç§’")
        return result
    except Exception as e:
        logger.error(f"ç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
        raise

# æœ¬åœ°RAGé—®ç­”ä¸»å‡½æ•°
def local_rag_qa(question, use_streaming=False, enable_memory=True):
    """æœ¬åœ°RAGé—®ç­”ä¸»å‡½æ•°"""
    try:
        # æ£€æŸ¥ç¼“å­˜
        if cache and cache.get(question):
            cached_result = cache.get(question)
            show_progress("ä½¿ç”¨ç¼“å­˜å›ç­”")
            return cached_result
        
        show_progress(f"å¼€å§‹å¤„ç†é—®é¢˜: {question}")
        
        # 1. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        show_progress("[æ­¥éª¤1/6] æ­£åœ¨åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...")
        embeddings = initialize_embeddings()
        
        # 2. åŠ è½½å‘é‡æ•°æ®åº“
        show_progress("[æ­¥éª¤2/6] æ­£åœ¨åŠ è½½å‘é‡æ•°æ®åº“...")
        db = load_vector_db(embeddings)
        
        # 3. åˆå§‹åŒ–è¯­è¨€æ¨¡å‹
        show_progress("[æ­¥éª¤3/6] æ­£åœ¨åˆå§‹åŒ–è¯­è¨€æ¨¡å‹...")
        llm = initialize_llm()
        
        # 4. åˆ›å»ºä¼šè¯å†…å­˜ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        memory = None
        if enable_memory and CONVERSATION_CONFIG.get("memory_type"):
            show_progress("[æ­¥éª¤4/6] æ­£åœ¨åˆ›å»ºä¼šè¯å†…å­˜...")
            memory = create_conversation_memory()
        else:
            show_progress("[æ­¥éª¤4/6] è·³è¿‡ä¼šè¯å†…å­˜åˆ›å»ºï¼ˆæœªå¯ç”¨ï¼‰")
        
        # 5. åˆ›å»ºé—®ç­”é“¾
        show_progress("[æ­¥éª¤5/6] æ­£åœ¨åˆ›å»ºé—®ç­”é“¾...")
        qa_chain = create_qa_chain(db, llm, memory)
        
        # 6. æ‰§è¡Œæ£€ç´¢ï¼ˆå¯é€‰ï¼Œç”¨äºè°ƒè¯•ï¼‰
        show_progress("[æ­¥éª¤6/6] æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£...")
        retriever = db.as_retriever(
            search_type=RETRIEVAL_CONFIG["search_type"],
            search_kwargs={"k": RETRIEVAL_CONFIG["k"]}
        )
        relevant_docs = perform_retrieval(retriever, question)
        
        # 7. ç”Ÿæˆå›ç­”
        # if use_streaming:
        #     show_progress("æ­£åœ¨æµå¼ç”Ÿæˆå›ç­”...")
        #     # æµå¼è¾“å‡ºæ¨¡å¼
        #     full_answer = ""
        #     print("\nAI å›ç­”ï¼š")
        #     print("=" * 50)
            
        #     for chunk in generate_answer_stream(qa_chain, question):
        #         print(chunk, end="", flush=True)
        #         full_answer += chunk
            
        #     print("\n" + "=" * 50)
            
        #     # æ„é€ ç»“æœå¯¹è±¡
        #     result = {
        #         "result": full_answer,
        #         "source_documents": relevant_docs
        #     }
    # åœ¨äº¤äº’å¼é—®ç­”çš„â€œæ‰“å°å›ç­”â€éƒ¨åˆ†ä¿®æ”¹
# åœ¨äº¤äº’å¼é—®ç­”çš„â€œæ‰“å°å›ç­”â€éƒ¨åˆ†ä¿®æ”¹
        if use_streaming:
            full_answer = ""
            print("\n" + "=#+"*60)
            print("ğŸ¯ AI å›ç­”ï¼ˆæµå¼è¾“å‡ºï¼‰ï¼š")
            print("-"*60)
            
            for chunk in generate_answer_stream(qa_chain, question):
                print(chunk, end="", flush=True)
                full_answer += chunk
            
            print("\n" + "="*60 + "\n")
            result = {"result": full_answer, "source_documents": relevant_docs}
        else:
            # å¸¸è§„æ¨¡å¼
            show_progress("æ­£åœ¨ç”Ÿæˆå›ç­”...")
            result = generate_answer(qa_chain, question)
        
        # ç¼“å­˜ç»“æœ
        if cache:
            cache.set(question, result)
        
        show_progress("é—®é¢˜å¤„ç†å®Œæˆ")
        return result
        
    except KeyboardInterrupt:
        error_msg = "æ“ä½œè¢«ç”¨æˆ·ä¸­æ–­"
        show_progress(f"é”™è¯¯: {error_msg}")
        logger.warning(error_msg)
        return {
            "result": error_msg,
            "source_documents": []
        }
    except Exception as e:
        error_msg = f"å¤„ç†é—®é¢˜æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        show_progress(f"é”™è¯¯: {error_msg}")
        logger.error(error_msg, exc_info=True)
        # è¿”å›é”™è¯¯ä¿¡æ¯ï¼Œä¾¿äºè°ƒç”¨è€…å¤„ç†
        return {
            "result": error_msg,
            "source_documents": []
        }

# äº¤äº’å¼é—®ç­”å‡½æ•°
def interactive_qa():
    """äº¤äº’å¼é—®ç­”å‡½æ•°"""
    print("\n" + "=" * 50)
    print("æ¬¢è¿ä½¿ç”¨RAGé—®ç­”ç³»ç»Ÿ")
    print("æ‚¨å¯ä»¥è¾“å…¥é—®é¢˜è¿›è¡Œé—®ç­”ï¼Œè¾“å…¥'exit'æˆ–'quit'é€€å‡º")
    print("è¾“å…¥'clear'æ¸…ç©ºç¼“å­˜")
    print("=" * 50 + "\n")
    
    # å¯åŠ¨æ—¶æ¸…ç©ºç¼“å­˜ä»¥é¿å…ä¹‹å‰æµ‹è¯•é—®é¢˜çš„å½±å“
    if cache:
        cache.cache = {}
        print("ç¼“å­˜å·²åœ¨å¯åŠ¨æ—¶æ¸…ç©ºï¼Œç¡®ä¿ä½¿ç”¨æ–°é—®é¢˜")
    
    while True:
        try:
            question = input("\nè¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ")
            
            # å¤„ç†å‘½ä»¤
            if question.lower() in ["exit", "quit", "é€€å‡º"]:
                print("è°¢è°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                break
            elif question.lower() == "clear":
                if cache:
                    # æ¸…ç©ºç¼“å­˜
                    cache.cache = {}
                    print("ç¼“å­˜å·²æ¸…ç©º")
                else:
                    print("ç¼“å­˜æœªå¯ç”¨")
                continue
            elif not question.strip():
                print("è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜")
                continue
            
            # å¤„ç†é—®é¢˜
            print(f"\næ­£åœ¨å¤„ç†é—®é¢˜: {question}")
            result = local_rag_qa(question, use_streaming=True, enable_memory=True)
            
            # æ‰“å°å‚è€ƒçš„æ–‡æ¡£ç‰‡æ®µ
            if result.get("source_documents"):
                print("\nå‚è€ƒçš„æ–‡æ¡£ç‰‡æ®µï¼š")
                for i, doc in enumerate(result["source_documents"], 1):
                    source = doc.metadata.get("source", "æœªçŸ¥æ¥æº")
                    file_type = doc.metadata.get("file_type", "æœªçŸ¥ç±»å‹")
                    print(f"\nç¬¬ {i} ä¸ªç‰‡æ®µ [æ¥æº: {os.path.basename(source)} ({file_type})]:")
                    # print(f"{doc.page_content[:200]}..." if len(doc.page_content) > 200 else doc.page_content)
                    print(f"{doc.page_content[:15]}..." if len(doc.page_content) > 15 else doc.page_content)
        except Exception as e:
            print(f"äº¤äº’è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            logger.error(f"äº¤äº’é”™è¯¯: {str(e)}", exc_info=True)

# ä¸»å‡½æ•°ï¼šæµ‹è¯•å¹¶æ‰“å°ç»“æœ
def main():
    try:
        import sys
        # æ£€æŸ¥æ˜¯å¦ä»¥äº¤äº’å¼æ¨¡å¼è¿è¡Œ
        if len(sys.argv) > 1 and sys.argv[1] == "--interactive":
            # äº¤äº’å¼æ¨¡å¼
            interactive_qa()
        elif len(sys.argv) > 1:
            # ä»å‘½ä»¤è¡Œè·å–é—®é¢˜
            question = " ".join(sys.argv[1:])
            show_progress(f"ç”¨æˆ·é—®é¢˜ï¼š{question}")
            
            # æ‰§è¡Œé—®ç­”
            show_progress("å¼€å§‹RAGæµç¨‹å¤„ç†ï¼Œè¯·ç¨å€™...")
            result = local_rag_qa(question, use_streaming=True)

            # æ‰“å°å‚è€ƒçš„æ–‡æ¡£ç‰‡æ®µ
            if result.get("source_documents"):
                print("\nå‚è€ƒçš„æ–‡æ¡£ç‰‡æ®µï¼š")
                for i, doc in enumerate(result["source_documents"], 1):
                    source = doc.metadata.get("source", "æœªçŸ¥æ¥æº")
                    print(f"\nç¬¬ {i} ä¸ªç‰‡æ®µ [æ¥æº: {source}]:")
                    print(f"{doc.page_content[:200]}..." if len(doc.page_content) > 200 else doc.page_content)
        else:
            # é»˜è®¤æ¨¡å¼ï¼Œæç¤ºç”¨æˆ·ä½¿ç”¨æ–¹æ³•
            print("\nä½¿ç”¨æ–¹æ³•ï¼š")
            print("  1. äº¤äº’å¼é—®ç­”: python script.py --interactive")
            print("  2. ç›´æ¥æé—®: python script.py ä½ çš„é—®é¢˜")
    
    except KeyboardInterrupt:
        print("\næ“ä½œå·²ä¸­æ–­")
    except Exception as e:
        error_msg = f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        show_progress(f"é”™è¯¯: {error_msg}")
        print(f"æ‰§è¡Œå¤±è´¥: {error_msg}")
        logger.error(error_msg, exc_info=True)

# ç¨‹åºå…¥å£
if __name__ == "__main__":
    main()
        